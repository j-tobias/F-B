{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## 1. The Algorithm\n",
    "\n",
    "The forward-backward splitting method is an iterative method for solving convex optimization problems. The method works by alternating between two steps:\n",
    "\n",
    "1. Forward step: The forward step moves the iterate in the direction of the gradient of the objective function.\n",
    "\n",
    "2. Backward step: The backward step projects the iterate onto the set of feasible solutions.\n",
    "\n",
    "The forward-backward splitting method is a relatively simple method, but it can be very effective for solving a wide variety of optimization problems. The method is particularly well-suited for problems that involve both an objective function and constraints.\n",
    "\n",
    "Here is an example of how the forward-backward splitting method can be used to solve the following optimization problem:\n",
    "\n",
    "$ min f(x) + g(x) $\n",
    "\n",
    "where $f$ is a convex function and g is a convex function that is defined on a convex set C. The forward-backward splitting method would work as follows:\n",
    "\n",
    "1. We would start with an initial iterate $ x_0 $​\t\n",
    "\n",
    "2. We would compute the gradient of $f$ at $x_0$ \n",
    "\n",
    "3. We would update the iterate to $ y = x_0 - \\alpha \\nabla f(x_0)$ \n",
    "\n",
    "4. We would compute the proximal operator of $g$ at $y$.\n",
    "\n",
    "5. We would update the iterate to $x_1 = y − \\alpha * prox_g(y) $\n",
    "\n",
    "The forward step would then be repeated, and the process would continue until a stopping criterion is reached.\n",
    "\n",
    "The forward-backward splitting method is a powerful tool that can be used to solve a wide variety of optimization problems. It is often used to solve problems that involve both an objective function and constraints.\n",
    "\n",
    "In this case $f(x)$ is given by $ 0.5 * ||Mx - y||_2 $ and $g(x)$ is given by $ \\tau * ||x||_1 $\n",
    "\n",
    "### 1.1 The Forward Step\n",
    "\n",
    "The forward step in the forward-backward splitting method moves the iterate in the direction of the gradient of the objective function. This means that the forward step is essentially a gradient descent step. The gradient of the objective function points in the direction of the steepest descent, so the forward step moves the iterate in the direction of the minimum of the objective function.\n",
    "\n",
    "In a nutshell, the forward step is a way of moving the iterate towards the minimum of the objective function by following the gradient of the objective function.\n",
    "\n",
    "### 1.2 The Backward Step\n",
    "\n",
    "The backward step in the forward-backward splitting method ensures that the iterate remains within the feasible set. This means that the backward step ensures that the iterate does not violate any of the constraints of the optimization problem.\n",
    "\n",
    "In a nutshell, the backward step is a way of ensuring that the iterate remains within the feasible set by projecting the iterate onto the feasible set.\n",
    "\n",
    "Here is an example of how the backward step works. Let's say we are trying to solve the following optimization problem:\n",
    "\n",
    "$min f(x)$\n",
    "\n",
    "$s.t. x >= 0$\n",
    "\n",
    "where f is a convex function. The feasible set for this problem is the set of all non-negative real numbers. The backward step would ensure that the iterate remains within the feasible set by projecting the iterate onto the feasible set. This means that the backward step would set all the negative elements of the iterate to zero.\n",
    "\n",
    "## 2. Mathematical Part\n",
    "\n",
    "### 2.1 The Objective\n",
    "\n",
    "The Objective\n",
    "\n",
    "$$ f(M, x, y, \\tau) =  0.5 * ||Mx - y||_2 +  \\tau * ||x||_1 $$\n",
    "\n",
    "I should also state the properties of the different variables:\n",
    "\n",
    "- $M$ is defined as a Matrix with $m$ rows and $n = 2m$ rows \n",
    "- $y$ is defined as a vector $y \\in \\mathbb{R}^{m}$\n",
    "- $\\tau$ is defined as $\\tau > 0$\n",
    "- $x$ is defined as $y \\in \\mathbb{R}^{n}$\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 The Gradient\n",
    "\n",
    "For the Forward Step the Gradient of $f(x)$ is needed. $ \\frac{1}{2} * M^T * \\frac{Mx - y}{|| Mx - y||_2}$\n",
    "\n",
    "![image](Gradient.png)\n",
    "\n",
    "*// this is the result of **plot_gradient v1.py***\n",
    "\n",
    "![image](Gradient_v2.png)\n",
    "\n",
    "*// this is the result of **plot_gradient v2.py***\n",
    "\n",
    "### 2.3 The Proximal Operator\n",
    "\n",
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Mathematical Parts\n",
    "\n",
    "### 3.2 The Algorithm\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
